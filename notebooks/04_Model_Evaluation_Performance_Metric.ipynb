{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing to the main directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/isham/Desktop/machine-learning-projects/fine-tuning-q-and-a\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from utils import BASE_MODEL_PATH, TRAINING_PATH, FINAL_MODEL_PATH, PROCESSED_DATA_DIR\n",
    "from utils import EPOCHS, LR, BATCH_SIZE, SAVE_TOTAL_LIMIT, EVALUATION_STRATEGY\n",
    "\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "from utils import clear_gpu_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tokenized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_data = load_from_disk(os.path.join(PROCESSED_DATA_DIR, \"train_tokenized_data\"))\n",
    "val_tokenized_data = load_from_disk(os.path.join(PROCESSED_DATA_DIR, \"val_tokenized_data\"))\n",
    "test_tokenized_data = load_from_disk(os.path.join(PROCESSED_DATA_DIR, \"test_tokenized_data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=TRAINING_PATH,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=EVALUATION_STRATEGY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Results Before Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_original_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=loaded_original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_data,\n",
    "    eval_dataset=val_tokenized_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='267' max='267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [267/267 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 42.816688537597656, 'eval_runtime': 12.8824, 'eval_samples_per_second': 82.826, 'eval_steps_per_second': 20.726}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(eval_dataset=test_tokenized_data)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loaded_original_model\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Results After Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(FINAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=trained_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_data,\n",
    "    eval_dataset=val_tokenized_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='267' max='267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [267/267 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16271285712718964, 'eval_runtime': 12.6429, 'eval_samples_per_second': 84.395, 'eval_steps_per_second': 21.118}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(eval_dataset=test_tokenized_data)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trained_model\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Before fine-tuning, the original flan-t5 model exhibits a higher evaluation loss of 42.8166, which generally indicates that the model's predictions were less accurate when compared to the validation dataset. The evaluation runtime stands at 12.8824 seconds, and the model processes approximately 82.826 samples per second.\n",
    "\n",
    "After fine-tuning, the trained flan-t5 model shows a significantly reduced evaluation loss of 0.1627, suggesting a considerable improvement in prediction accuracy and a closer match to the expected outputs in the validation data. The runtime for evaluation is slightly higher at 12.6429 seconds, yet the model processes a higher rate of 84.395 samples per second.\n",
    "\n",
    "The improvement in loss post fine-tuning reflects a more precise model, likely due to the model learning from the domain-specific nuances in the fine-tuning process. The consistent processing speed before and after fine-tuning, despite the complexity of the model having potentially increased, indicates an efficient use of computational resources. This comparison underlines the importance of fine-tuning in tailoring a language model to a specific context, resulting in more accurate and reliable predictions.\n",
    "\n",
    "In the following section, we will use samples from test dataset of actual answers with those generated by the model to assess its performance more comprehensively. While evaluation loss provides a quantitative measure of the model's accuracy, it does not encapsulate the full scope of the model's capabilities. A qualitative comparison will give us better insight into the model's practical effectiveness and its ability to produce coherent and contextually relevant answers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-q-and-a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
